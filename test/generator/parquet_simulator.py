#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
E-commerce Platform Data Simulator for Parquet Files

This script generates synthetic e-commerce platform data and saves it as Parquet files.
It can be configured to create files of various sizes by adjusting the number of rows,
columns, row groups, and page sizes.

Author: Yang Tian (data content is generated by Deepseek)
Date: 2025/4/13 
"""

import os
import sys
import time
import random
import uuid
import argparse
import gc
import concurrent.futures
import multiprocessing
from multiprocessing import Pool, cpu_count
import tempfile
from datetime import datetime, timedelta
import math
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.fs as fs
import shutil
import json
import multiprocessing.shared_memory as shared_memory
import pickle
import warnings
import psutil
import glob

# Global variables with default values - will be set by command line arguments
RANDOM_SEED = 42
NUM_PROCESSES = multiprocessing.cpu_count()
MEMORY_OPTIMIZATION = False  # Define a global parameter for shared memory usage

# Helper functions for PyArrow operations
def create_arrow_table(data_dict):
    """
    Create an Arrow table from a dictionary of columns.
    
    Args:
        data_dict: Dictionary with column names as keys and lists/arrays as values
    
    Returns:
        PyArrow Table object
    """
    fields = []
    arrays = []
    
    for col_name, values in data_dict.items():
        # Convert numpy arrays to PyArrow arrays
        if isinstance(values, np.ndarray):
            if values.dtype == np.dtype('O') and isinstance(values[0], str):
                # For string arrays
                arr = pa.array(values, type=pa.string())
            else:
                # For numeric arrays
                arr = pa.array(values)
        else:
            # For python lists
            arr = pa.array(values)
            
        field = pa.field(col_name, arr.type)
        fields.append(field)
        arrays.append(arr)
    
    schema = pa.schema(fields)
    return pa.Table.from_arrays(arrays, schema=schema)

def read_parquet_to_arrow(file_path):
    """
    Read a Parquet file and return as PyArrow table.
    
    Args:
        file_path: Path to the Parquet file
    
    Returns:
        PyArrow Table object
    """
    return pq.read_table(file_path)

def write_arrow_to_parquet(table, file_path):
    """
    Write PyArrow table to Parquet file without compression.
    
    Args:
        table: PyArrow Table object
        file_path: Path where to write the Parquet file
    
    Returns:
        file_path: Path to the written Parquet file
    """
    pq.write_table(table, file_path, compression=None)
    return file_path

def merge_parquet_files(file_paths, output_path):
    """
    Merge multiple Parquet files into a single file.
    
    Args:
        file_paths: List of paths to Parquet files
        output_path: Path to output Parquet file
    
    Returns:
        output_path: Path to the merged Parquet file
    """
    if not file_paths:
        raise ValueError("No files to merge")
    
    # Read and concatenate all tables
    tables = [pq.read_table(file_path) for file_path in file_paths]
    merged_table = pa.concat_tables(tables)
    
    # Write the merged table to output path
    pq.write_table(merged_table, output_path, compression=None)
    
    return output_path

# Create directory if it does not exist
def create_directory_if_not_exists(dir_path):
    """
    Create directory if it does not exist
    
    Parameters:
        dir_path: Path to the directory to create
    """
    if not os.path.exists(dir_path):
        os.makedirs(dir_path, exist_ok=True)
        print(f"Created directory: {dir_path}")
    return dir_path

# Merge multiple parquet files into a single output file
def merge_files(temp_files, output_file):
    """
    Optimized function to merge multiple parquet files into a single output file
    No compression is used to maximize speed
    
    Parameters:
        temp_files: List of temporary file paths
        output_file: Output file path
        
    Returns:
        int: Total number of rows in the merged file
    """
    if not temp_files:
        return 0
        
    # Create output directory if not exists
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    # Instead of loading all tables in memory, use pyarrow's dataset capabilities
    # to concatenate and write in a streaming fashion
    print(f"Merging {len(temp_files)} files using optimized method...")
    
    # Get the schema from the first file
    try:
        schema = pq.read_schema(temp_files[0])
        
        # Use ParquetWriter for better performance, but WITHOUT compression
        with pq.ParquetWriter(output_file, schema, compression=None) as writer:
            total_rows = 0
            # Process files in smaller batches to reduce memory consumption
            batch_size = min(50, len(temp_files))
            for i in range(0, len(temp_files), batch_size):
                batch_files = temp_files[i:i+batch_size]
                
                for file_path in batch_files:
                    if os.path.exists(file_path):
                        try:
                            # Read and write in a streaming fashion
                            reader = pq.ParquetFile(file_path)
                            for batch in reader.iter_batches(batch_size=100000):
                                writer.write_batch(batch)
                                total_rows += len(batch)
                        except Exception as e:
                            print(f"Error processing temp file {file_path}: {e}")
                        finally:
                            # Delete temporary file
                            try:
                                os.remove(file_path)
                            except:
                                pass
                
                # Force garbage collection after each batch
                gc.collect()
        
        return total_rows
        
    except Exception as e:
        print(f"Error in optimized merge: {e}")
        # Fall back to original implementation if the optimized version fails
        return _merge_files_original(temp_files, output_file)

# Keep original merge implementation as fallback
def _merge_files_original(temp_files, output_file):
    """Original merge implementation as fallback"""
    # Read and concatenate all tables
    tables = []
    total_rows = 0
    
    for file_path in temp_files:
        if os.path.exists(file_path):
            try:
                table = pq.read_table(file_path)
                tables.append(table)
                total_rows += table.num_rows
            except Exception as e:
                print(f"Error reading temp file {file_path}: {e}")
            finally:
                # Delete temporary file
                try:
                    os.remove(file_path)
                except:
                    pass
    
    if not tables:
        return 0
    
    # Concatenate tables
    merged_table = pa.concat_tables(tables)
    
    # Write to output file WITHOUT compression
    pq.write_table(
        merged_table,
        output_file,
        compression=None
    )
    
    return total_rows

def merge_and_append_parquet(temp_files, output_file, mode='write'):
    """
    Merge multiple parquet files, with option to overwrite or append to existing file
    No compression is used to maximize performance
    
    Parameters:
        temp_files: List of temporary file paths
        output_file: Output file path
        mode: 'write' to overwrite, 'append' to append to existing file
        
    Returns:
        int: Total number of rows processed
    """
    if not temp_files:
        return 0
    
    # Create output directory if it doesn't exist
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    # Read and merge all temporary tables
    tables = []
    total_rows = 0
    
    for file_path in temp_files:
        if os.path.exists(file_path):
            try:
                table = pq.read_table(file_path)
                tables.append(table)
                total_rows += table.num_rows
            except Exception as e:
                print(f"Error reading temp file {file_path}: {e}")
            finally:
                # Delete temporary file
                try:
                    os.remove(file_path)
                except Exception as e:
                    print(f"Warning: Could not delete temp file {file_path}: {e}")
    
    if not tables:
        print("No valid temp files to merge")
        return 0
    
    # Merge temporary tables
    merged_table = pa.concat_tables(tables)
    
    # Decide how to write based on mode
    if mode == 'write' or not os.path.exists(output_file):
        # Overwrite mode or file doesn't exist, directly write
        pq.write_table(
            merged_table,
            output_file,
            compression=None
        )
        print(f"Wrote {merged_table.num_rows} rows to {output_file}")
    else:
        # Append mode
        try:
            # If file exists, read existing file
            existing_table = pq.read_table(output_file)
            
            # Merge existing file with new data
            combined_table = pa.concat_tables([existing_table, merged_table])
            
            # Write back to file
            pq.write_table(
                combined_table,
                output_file,
                compression=None
            )
            print(f"Appended {merged_table.num_rows} rows to {output_file}, new total: {combined_table.num_rows} rows")
        except Exception as e:
            # If reading existing file fails, directly overwrite
            print(f"Warning: Could not append to existing file, overwriting instead: {e}")
            pq.write_table(
                merged_table,
                output_file,
                compression=None
            )
    
    # Clean up memory
    del tables
    del merged_table
    if 'existing_table' in locals():
        del existing_table
    if 'combined_table' in locals():
        del combined_table
    gc.collect()
    
    return total_rows

# Print current memory usage
def print_memory_usage(stage=""):
    """Print current memory usage"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    memory_mb = memory_info.rss / (1024 * 1024)
    if stage:
        print(f"Memory usage {stage}: {memory_mb:.2f} MB")
    else:
        print(f"Memory usage: {memory_mb:.2f} MB")

# Parallel processing parameters
CPU_COUNT = cpu_count()
# Use at most CPU count-1, at least 1
NUM_PROCESSES = max(1, min(CPU_COUNT - 1, 8))  # Limit to 8 processes maximum to avoid resource contention
# Thread count for IO-intensive tasks
NUM_THREADS = max(4, min(CPU_COUNT * 2, 16))  # IO tasks can use more threads

print(f"System CPU count: {CPU_COUNT}")
print(f"Will use {NUM_PROCESSES} processes for parallel data generation")
print(f"Will use {NUM_THREADS} threads for IO operations")

# Initialize global random seed
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# Initialize random seed for each process
def init_worker(seed):
    """Initialize random seed for worker process"""
    np.random.seed(seed)
    random.seed(seed)

# Pre-generate some common data pools for reuse
def generate_name_pools(seed=42):
    """Generate pools of common data for reuse across processes"""
    random.seed(seed)
    
    # Common first names
    first_names = ['James', 'Mary', 'John', 'Patricia', 'Robert', 'Jennifer', 'Michael', 'Linda', 'William', 
                  'Elizabeth', 'David', 'Barbara', 'Richard', 'Susan', 'Joseph', 'Jessica', 'Thomas', 'Sarah', 
                  'Charles', 'Karen', 'Daniel', 'Lisa', 'Matthew', 'Nancy', 'Anthony', 'Betty', 'Mark', 'Sandra', 
                  'Donald', 'Margaret', 'Steven', 'Ashley', 'Paul', 'Emily', 'Andrew', 'Donna', 'Joshua', 'Michelle', 
                  'Kenneth', 'Carol', 'Kevin', 'Amanda', 'Brian', 'Melissa', 'George', 'Deborah', 'Timothy', 'Stephanie']
    
    # Common last names
    last_names = ['Smith', 'Johnson', 'Williams', 'Jones', 'Brown', 'Davis', 'Miller', 'Wilson', 'Moore', 'Taylor', 
                 'Anderson', 'Thomas', 'Jackson', 'White', 'Harris', 'Martin', 'Thompson', 'Garcia', 'Martinez', 
                 'Robinson', 'Clark', 'Rodriguez', 'Lewis', 'Lee', 'Walker', 'Hall', 'Allen', 'Young', 'Hernandez', 
                 'King', 'Wright', 'Lopez', 'Hill', 'Scott', 'Green', 'Adams', 'Baker', 'Gonzalez', 'Nelson', 'Carter', 
                 'Mitchell', 'Perez', 'Roberts', 'Turner', 'Phillips', 'Campbell', 'Parker', 'Evans', 'Edwards', 'Collins']
    
    # Common street names
    street_names = ['Main', 'High', 'Park', 'Oak', 'Pine', 'Maple', 'Cedar', 'Elm', 'Washington', 'Lake', 
                   'Hill', 'Sunset', 'Lincoln', 'Church', 'River', 'Valley', 'Bridge', 'Canal', 'Forest', 
                   'Meadow', 'Spring', 'Market', 'Broadway', 'First', 'Second', 'Third', 'Fourth', 'Fifth']
    
    # Common words for product names
    common_words = ['Premium', 'Ultra', 'Super', 'Deluxe', 'Basic', 'Classic', 'Modern', 'Traditional', 'Professional', 
                   'Standard', 'Advanced', 'Essential', 'Quality', 'Value', 'Luxury', 'Budget', 'Smart', 'Ergonomic', 
                   'Compact', 'Portable', 'Wireless', 'Digital', 'Analog', 'Hybrid', 'Solar', 'Electric', 'Manual', 
                   'Automatic', 'Rechargeable', 'Eco', 'Natural', 'Synthetic', 'Handmade', 'Custom', 'Adjustable', 
                   'Foldable', 'Expandable', 'Retractable', 'Versatile', 'Multipurpose', 'Lightweight', 'Heavy-duty', 
                   'Waterproof', 'Weatherproof', 'Shockproof', 'Durable', 'Sturdy', 'Elegant', 'Sleek', 'Invisible']
    
    return {
        'first_names': first_names,
        'last_names': last_names,
        'street_names': street_names,
        'common_words': common_words
    }

# Common data pools
NAME_POOLS = generate_name_pools()

# Create shared configuration parameters to ensure all processes use the same settings
def create_shared_config():
    return {
        'categories': ['Electronics', 'Clothing', 'Home & Kitchen', 'Beauty', 'Books', 
                     'Sports', 'Toys', 'Grocery', 'Health', 'Automotive'],
        'brands': [
            'TechMaster', 'EliteGear', 'SmartLife', 'PrimeBrand', 'SuperValue', 'QualityFirst', 'TopShelf', 
            'NextGen', 'InnovateX', 'GloBest', 'PowerTech', 'HomeEssentials', 'ComfortZone', 'UrbanLiving', 
            'NaturalChoice', 'PerfectFit', 'EcoSmart', 'ModernStyle', 'ClassicDesign', 'BudgetBuy'
        ] * 5,  # Multiply to create 100 brands
        'payment_methods': ['Credit Card', 'PayPal', 'Bank Transfer', 'Apple Pay', 'Google Pay'],
        'statuses': ['Completed', 'Shipped', 'Processing', 'Cancelled', 'Refunded', 'Pending'],
        'status_probabilities': [0.6, 0.15, 0.1, 0.05, 0.05, 0.05],
        'warehouse_locations': [
            "WH-EAST-NY", "WH-WEST-CA", "WH-CENTRAL-TX", "WH-SOUTH-FL", 
            "WH-NORTH-IL", "WH-PACIFIC-WA", "WH-MOUNTAIN-CO", "WH-EUROPE-UK"
        ],
        'rating_probabilities': [0.05, 0.10, 0.15, 0.30, 0.40],  # Probabilities for 1-5 stars
        'verified_purchase_prob': 0.85,
        'adjectives': NAME_POOLS['common_words'][:25],
        'nouns': NAME_POOLS['common_words'][25:50],
        'features': NAME_POOLS['common_words'][50:],
        'positive_titles': [
            "Love this product!", "Exceeded expectations", "Highly recommend", "Absolutely worth it", "Great quality",
            "Surprisingly good", "Perfect for my needs", "Glad I bought this", "Very satisfied",
            "Excellent quality", "Love it so much", "Great value", "Very practical", "Flawless"
        ],
        'neutral_titles': [
            "It's okay", "Meets expectations", "Satisfactory", "Average", "Acceptable",
            "Decent quality", "Standard product", "Fair price", "Basic functionality"
        ],
        'negative_titles': [
            "Not recommended", "Disappointed", "Waste of money", "Below expectations", "Poor quality",
            "Regret purchasing", "Better options available", "Not worth the price", "Has issues"
        ]
    }

# Global shared configuration
SHARED_CONFIG = create_shared_config()

# Worker process function - product data generation
def worker_generate_products(args):
    """
    Worker process function, generate a batch of product data
    
    Args:
        args: Tuple containing (batch_id, start_idx, batch_size, temp_dir)
        
    Returns:
        str: Path to the saved temporary Parquet file
    """
    batch_id, start_idx, batch_size, temp_dir = args
    
    # Record start time
    start_time = time.time()
    print(f"Process {os.getpid()} starting product batch #{batch_id}: {start_idx} to {start_idx + batch_size}")
    
    # Access shared configuration
    categories = SHARED_CONFIG['categories']
    brands = SHARED_CONFIG['brands']
    adjectives = SHARED_CONFIG['adjectives']
    
    # Create arrays for each column
    product_ids = np.arange(start_idx, start_idx + batch_size).astype(str)
    product_ids = np.array([f"PROD-{pid}" for pid in product_ids])
    
    # Generate category and subcategory data
    category_indices = np.random.randint(0, len(categories), batch_size)
    categories_array = np.array(categories)[category_indices]
    
    # Define subcategories for each category
    subcategories_by_category = {
        'Electronics': ['Smartphones', 'Laptops', 'Tablets', 'TVs', 'Cameras', 'Audio', 'Wearables'],
        'Clothing': ['Men', 'Women', 'Children', 'Shoes', 'Accessories', 'Activewear', 'Outerwear'],
        'Home & Kitchen': ['Furniture', 'Appliances', 'Cookware', 'Bedding', 'Decor', 'Lighting', 'Storage'],
        'Beauty': ['Skincare', 'Makeup', 'Haircare', 'Fragrance', 'Bath & Body', 'Tools', 'Sets'],
        'Books': ['Fiction', 'Non-fiction', 'Children', 'Textbooks', 'Comics', 'Reference', 'Magazines'],
        'Sports': ['Fitness', 'Outdoor', 'Team Sports', 'Water Sports', 'Winter Sports', 'Apparel', 'Equipment'],
        'Toys': ['Games', 'Educational', 'Dolls', 'Action Figures', 'Vehicles', 'Arts & Crafts', 'Puzzles'],
        'Grocery': ['Pantry', 'Snacks', 'Beverages', 'Canned Goods', 'Fresh', 'Frozen', 'Organic'],
        'Health': ['Vitamins', 'Supplements', 'Personal Care', 'First Aid', 'OTC Medication', 'Medical Devices', 'Wellness'],
        'Automotive': ['Parts', 'Accessories', 'Tools', 'Electronics', 'Interior', 'Exterior', 'Maintenance']
    }
    
    # Generate subcategories based on the selected category
    subcategories_array = np.array([
        np.random.choice(subcategories_by_category[categories_array[i]]) 
        for i in range(batch_size)
    ])
    
    # Generate brand data
    brand_indices = np.random.randint(0, len(brands), batch_size)
    brands_array = np.array(brands)[brand_indices]
    
    # Generate product names using adjectives and category names
    product_names = np.array([
        f"{np.random.choice(adjectives)} {brands_array[i]} {subcategories_array[i]}" 
        for i in range(batch_size)
    ])
    
    # Generate prices between $5 and $1000
    prices = np.round(np.random.exponential(scale=50, size=batch_size) + 5, 2)
    prices = np.clip(prices, 5, 1000)  # Limit to reasonable range
    
    # Generate inventory data
    stock_qty = np.random.randint(0, 1000, batch_size)
    reorder_level = np.random.randint(5, 100, batch_size)
    supplier_ids = np.array([f"SUP-{np.random.randint(1, 100):03d}" for _ in range(batch_size)])
    
    # Generate product attributes
    weight = np.round(np.random.uniform(0.1, 50.0, batch_size), 2)  # Weight in kg
    length = np.round(np.random.uniform(1, 200, batch_size), 1)  # Length in cm
    width = np.round(np.random.uniform(1, 100, batch_size), 1)  # Width in cm
    height = np.round(np.random.uniform(1, 100, batch_size), 1)  # Height in cm
    
    # Generate warehouse location
    warehouse_locations = np.random.choice(
        SHARED_CONFIG['warehouse_locations'], 
        size=batch_size
    )
    
    # Generate product listing details
    is_active = np.random.choice([True, False], size=batch_size, p=[0.95, 0.05])
    creation_date = np.array([
        (datetime.now() - timedelta(days=np.random.randint(1, 730))).strftime('%Y-%m-%d')
        for _ in range(batch_size)
    ])
    
    # Create dictionary for PyArrow table
    data_dict = {
        'product_id': product_ids,
        'product_name': product_names,
        'category': categories_array,
        'subcategory': subcategories_array,
        'brand': brands_array,
        'price': prices,
        'stock_quantity': stock_qty,
        'reorder_level': reorder_level,
        'supplier_id': supplier_ids,
        'weight_kg': weight,
        'length_cm': length,
        'width_cm': width,
        'height_cm': height,
        'warehouse_location': warehouse_locations,
        'is_active': is_active,
        'created_at': creation_date
    }
    
    # Create PyArrow table
    table = create_arrow_table(data_dict)
    
    # Create temporary directory if it doesn't exist
    os.makedirs(temp_dir, exist_ok=True)
    
    # Save to temporary file
    temp_file = os.path.join(temp_dir, f"products_batch_{batch_id}.parquet")
    write_arrow_to_parquet(table, temp_file)
    
    # Print progress
    elapsed = time.time() - start_time
    print(f"Completed product batch #{batch_id}: {batch_size} products in {elapsed:.2f}s ({batch_size/elapsed:.1f} items/s)")
    
    return temp_file

def generate_product_data(num_products=10000, num_processes=None, temp_dir=None):
    """
    Generate simulated product data using PyArrow.
    
    Args:
        num_products: Number of products to generate
        num_processes: Number of processes to use for generation
        temp_dir: Directory for temporary files
        
    Returns:
        tuple: (PyArrow Table, output file path)
    """
    print(f"Generating {num_products} products...")
    start_time = time.time()
    
    # Create temporary directory
    if temp_dir is None:
        temp_dir = os.path.join(tempfile.gettempdir(), "ecommerce_generator", "products")
    os.makedirs(temp_dir, exist_ok=True)
    
    # Set number of processes
    if num_processes is None:
        num_processes = min(NUM_PROCESSES, max(1, num_products // 10000))
    
    print(f"Using {num_processes} processes for product generation")
    
    # Calculate batch sizes
    batch_size = max(1000, min(100000, num_products // num_processes))
    num_batches = (num_products + batch_size - 1) // batch_size  # Ceiling division
    
    # Prepare arguments for each worker
    worker_args = []
    for i in range(num_batches):
        start_idx = i * batch_size
        # Adjust last batch size
        actual_batch_size = min(batch_size, num_products - start_idx)
        worker_args.append((i, start_idx, actual_batch_size, temp_dir))
    
    # Generate data in parallel
    temp_files = []
    if num_processes > 1 and num_batches > 1:
        with Pool(processes=min(num_processes, num_batches)) as pool:
            temp_files = pool.map(worker_generate_products, worker_args)
    else:
        # Single process execution
        temp_files = [worker_generate_products(args) for args in worker_args]
    
    # Filter out any None results
    temp_files = [f for f in temp_files if f is not None]
    
    # Merge all temporary files into one output file
    output_file = "./data/ecommerce_products.parquet"
    
    # Read and merge all temporary tables
    tables = []
    total_rows = 0
    
    for file_path in temp_files:
        if os.path.exists(file_path):
            try:
                table = read_parquet_to_arrow(file_path)
                tables.append(table)
                total_rows += table.num_rows
            except Exception as e:
                print(f"Error reading temp file {file_path}: {e}")
            finally:
                # Delete temporary file
                try:
                    os.remove(file_path)
                except Exception as e:
                    print(f"Warning: Could not delete temp file {file_path}: {e}")
    
    if not tables:
        print("No valid temp files to merge")
        return None, None
    
    # Merge all tables
    merged_table = pa.concat_tables(tables)
    
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # Write merged table to output file
    write_arrow_to_parquet(merged_table, output_file)
    
    elapsed = time.time() - start_time
    print(f"Generated {total_rows} products in {elapsed:.2f} seconds ({total_rows/elapsed:.1f} products/second)")
    print(f"Product data saved to {output_file}")
    
    # Check file size
    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)
    print(f"Product data file size: {file_size_mb:.2f} MB")
    
    return merged_table, output_file

def init_worker(seed):
    """Initialize random seed for worker process"""
    np.random.seed(seed)
    random.seed(seed)

# Worker process function for order data generation
def worker_generate_orders(args):
    """
    Worker function to generate a batch of order data
    
    Parameters:
        args: Tuple containing:
            batch_idx: Batch ID
            start_idx: Starting index
            batch_size: Batch size
            product_ids: Array of product IDs
            customer_ids: Array of customer IDs
            temp_dir: Directory for temporary files
            shipping_methods: Dictionary of shipping method configurations
            payment_methods: Dictionary of payment method configurations
            order_statuses: Dictionary of order status configurations
            avg_items_per_order: Average number of items per order
            
    Returns:
        tuple: (order_file_path, order_items_file_path)
    """
    batch_idx, start_idx, batch_size, product_ids, customer_ids, temp_dir, shipping_methods, payment_methods, order_statuses, avg_items_per_order = args
    
    print(f"Process {os.getpid()}: Starting order batch #{batch_idx+1} generation ({batch_size} orders)")
    start_time = time.time()
    
    # Generate order IDs
    order_ids = [f"ORD-{start_idx+i:010d}" for i in range(batch_size)]
    
    # Generate order dates (past 3 years)
    min_date = datetime.now() - timedelta(days=365*3)
    max_date = datetime.now()
    days_range = (max_date - min_date).days
    
    random_days = np.random.randint(0, days_range, batch_size)
    order_dates = np.array([min_date + timedelta(days=int(days)) for days in random_days])
    
    # Generate customer IDs
    selected_customers = np.random.choice(customer_ids, batch_size)
    
    # Generate payment methods
    payment_method_names = list(payment_methods.keys())
    payment_method_probabilities = [payment_methods[method] for method in payment_method_names]
    selected_payment_methods = np.random.choice(
        payment_method_names, 
        batch_size, 
        p=payment_method_probabilities
    )
    
    # Generate shipping methods
    shipping_method_names = list(shipping_methods.keys())
    shipping_method_probabilities = [shipping_methods[method]['probability'] for method in shipping_method_names]
    
    # Normalize probabilities if needed
    shipping_method_probabilities = np.array(shipping_method_probabilities)
    if shipping_method_probabilities.sum() != 1.0:
        shipping_method_probabilities = shipping_method_probabilities / shipping_method_probabilities.sum()
    
    selected_shipping_methods = np.random.choice(
        shipping_method_names, 
        batch_size, 
        p=shipping_method_probabilities
    )
    
    # Generate shipping costs based on shipping method
    shipping_costs = np.zeros(batch_size)
    for i, method in enumerate(selected_shipping_methods):
        min_cost, max_cost = shipping_methods[method]['cost_range']
        shipping_costs[i] = round(random.uniform(min_cost, max_cost), 2)
    
    # Generate order statuses
    status_names = list(order_statuses.keys())
    status_probabilities = [order_statuses[status] for status in status_names]
    
    # Normalize probabilities if needed
    status_probabilities = np.array(status_probabilities)
    if status_probabilities.sum() != 1.0:
        status_probabilities = status_probabilities / status_probabilities.sum()
    
    selected_statuses = np.random.choice(
        status_names, 
        batch_size, 
        p=status_probabilities
    )
    
    # Build order items list
    order_items = []
    order_subtotals = np.zeros(batch_size)
    
    # Get available product count
    product_count = len(product_ids)
    
    # Track total items for reporting
    total_items = 0
    
    # Generate order items with varying counts for each order
    for i in range(batch_size):
        # Vary number of items per order - skewed toward fewer items
        items_count = max(1, int(np.random.exponential(scale=avg_items_per_order - 0.5))) + 1
        items_count = min(items_count, 20)  # Cap at 20 items per order
        
        # Select random products for this order
        order_products = np.random.choice(product_ids, items_count)
        
        # Generate random quantities (1-5 items per product)
        quantities = np.random.randint(1, 6, items_count)
        
        # Generate random prices
        prices = np.round(np.random.uniform(5.0, 500.0, items_count), 2)
        
        # Add to order items
        for j in range(items_count):
            item_id = f"ITEM-{start_idx}-{i}-{j}"
            product_id = order_products[j]
            quantity = quantities[j]
            price = prices[j]
            subtotal = round(quantity * price, 2)
            
            order_items.append({
                'order_item_id': item_id,
                'order_id': order_ids[i],
                'product_id': product_id,
                'quantity': quantity,
                'price': price,
                'subtotal': subtotal
            })
            
            # Update order subtotal
            order_subtotals[i] += subtotal
        
        total_items += items_count
    
    # Calculate taxes (assume flat 8% tax rate)
    tax_rates = np.full(batch_size, 0.08)
    tax_amounts = np.round(order_subtotals * tax_rates, 2)
    
    # Calculate total amounts
    order_totals = np.round(order_subtotals + tax_amounts + shipping_costs, 2)
    
    # Create order data dictionary
    orders_data = {
        'order_id': order_ids,
        'customer_id': selected_customers,
        'order_date': order_dates,
        'payment_method': selected_payment_methods,
        'shipping_method': selected_shipping_methods,
        'shipping_cost': shipping_costs,
        'subtotal': order_subtotals,
        'tax_amount': tax_amounts,
        'total_amount': order_totals,
        'order_status': selected_statuses
    }
    
    # Create order items data grouped by columns
    order_items_data = {
        'order_item_id': [],
        'order_id': [],
        'product_id': [],
        'quantity': [],
        'price': [],
        'subtotal': []
    }
    
    for item in order_items:
        for key, value in item.items():
            order_items_data[key].append(value)
    
    # Create PyArrow arrays for orders
    orders_arrays = []
    orders_schema_fields = []
    
    for name, values in orders_data.items():
        array = pa.array(values)
        field = pa.field(name, array.type)
        orders_arrays.append(array)
        orders_schema_fields.append(field)
    
    # Create orders schema and table
    orders_schema = pa.schema(orders_schema_fields)
    orders_table = pa.Table.from_arrays(orders_arrays, schema=orders_schema)
    
    # Create PyArrow arrays for order items
    order_items_arrays = []
    order_items_schema_fields = []
    
    for name, values in order_items_data.items():
        array = pa.array(values)
        field = pa.field(name, array.type)
        order_items_arrays.append(array)
        order_items_schema_fields.append(field)
    
    # Create order items schema and table
    order_items_schema = pa.schema(order_items_schema_fields)
    order_items_table = pa.Table.from_arrays(order_items_arrays, schema=order_items_schema)
    
    # Create temp directory if it doesn't exist
    os.makedirs(temp_dir, exist_ok=True)
    
    # Write orders to temporary file
    orders_file = os.path.join(temp_dir, f"orders_batch_{batch_idx}.parquet")
    pq.write_table(orders_table, orders_file, compression=None)
    
    # Write order items to temporary file
    order_items_file = os.path.join(temp_dir, f"order_items_batch_{batch_idx}.parquet")
    pq.write_table(order_items_table, order_items_file, compression=None)
    
    # Track processing time
    proc_time = time.time() - start_time
    print(f"Process {os.getpid()}: Generated {batch_size} orders with {total_items} items in {proc_time:.2f}s " +
          f"({batch_size/proc_time:.1f} orders/s)")
    
    return (orders_file, order_items_file)

def generate_order_data(products_df, customers_df, num_orders=50000, avg_items_per_order=3):
    """
    Generate order data using parallel processing with shared memory for efficiency
    
    Parameters:
        products_df: Path to product data file or PyArrow Table
        customers_df: Path to customer data file or PyArrow Table
        num_orders: Number of orders to generate (default: 50000)
        avg_items_per_order: Average number of items per order (default: 3)
    
    Returns:
        Path to the generated orders parquet file
        Path to the generated order items parquet file
    """
    print(f"Using {NUM_PROCESSES} processes to generate {num_orders} orders in parallel...")
    
    # If paths are provided, load the data using PyArrow
    if isinstance(products_df, str):
        products_table = pq.read_table(products_df)
        product_ids = products_table.column('product_id').to_numpy()
    else:
        # Assume it's already a PyArrow Table
        products_table = products_df
        product_ids = products_table.column('product_id').to_numpy()
    
    if isinstance(customers_df, str):
        customers_table = pq.read_table(customers_df)
        customer_ids = customers_table.column('customer_id').to_numpy()
    else:
        # Assume it's already a PyArrow Table
        customers_table = customers_df
        customer_ids = customers_table.column('customer_id').to_numpy()
    
    # Define order statuses and their probabilities
    order_statuses = {
        'Completed': 0.70,
        'Processing': 0.10,
        'Shipped': 0.10,
        'Canceled': 0.05,
        'Returned': 0.05
    }
    
    # Define payment methods and their probabilities
    payment_methods = {
        'Credit Card': 0.55,
        'PayPal': 0.20,
        'Debit Card': 0.15,
        'Gift Card': 0.05,
        'Bank Transfer': 0.05
    }
    
    # Define shipping methods and their costs
    shipping_methods = {
        'Standard': {'cost_range': (3.99, 7.99), 'probability': 0.6},
        'Express': {'cost_range': (8.99, 14.99), 'probability': 0.3},
        'Next Day': {'cost_range': (15.99, 24.99), 'probability': 0.1}
    }
    
    # Create temporary directory for batch files
    temp_dir = os.path.join(os.getcwd(), "temp_orders")
    create_directory_if_not_exists(temp_dir)
    
    # Determine batch size
    batch_size = max(100000, num_orders // (NUM_PROCESSES * 2))
    num_batches = (num_orders + batch_size - 1) // batch_size
    
    print(f"Processing {num_batches} batches with approximately {batch_size} orders each")
    
    # Prepare process pool parameters
    args_list = []
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, num_orders)
        batch_size_actual = end_idx - start_idx
        
        args_list.append((
            batch_idx,
            start_idx,
            batch_size_actual,
            product_ids,
            customer_ids,
            temp_dir,
            shipping_methods,
            payment_methods,
            order_statuses,
            avg_items_per_order
        ))
    
    # Create process pool and execute parallel processing
    start_time = time.time()
    batch_files_pairs = []
    
    with Pool(processes=NUM_PROCESSES,
              initializer=init_worker,
              initargs=(RANDOM_SEED + 3000,)) as pool:
        # Execute parallel processing
        batch_files_pairs = pool.map(worker_generate_orders, args_list)
    
    # Separate order files and order item files
    batch_files_orders = [pair[0] for pair in batch_files_pairs if pair and len(pair) == 2]
    batch_files_items = [pair[1] for pair in batch_files_pairs if pair and len(pair) == 2]
    
    # Ensure output directory exists
    create_directory_if_not_exists('./data')
    
    # Final output paths
    orders_output_path = './data/ecommerce_orders.parquet'
    order_items_output_path = './data/ecommerce_order_items.parquet'
    
    # Merge all temporary files
    print(f"Merging {len(batch_files_orders)} order temporary files...")
    merge_time = time.time()
    
    # Merge order files
    orders_total_rows = merge_files(batch_files_orders, orders_output_path)
    
    # Merge order item files
    items_total_rows = merge_files(batch_files_items, order_items_output_path)
    
    merge_complete_time = time.time() - merge_time
    
    # Clean up temporary directory
    try:
        shutil.rmtree(temp_dir)
        print(f"Removed temporary directory: {temp_dir}")
    except Exception as e:
        print(f"Could not remove temporary directory: {e}")
    
    total_time = time.time() - start_time
    print(f"Generated {orders_total_rows} orders and {items_total_rows} order items in {total_time:.2f} seconds (merge: {merge_complete_time:.2f}s)")
    print(f"Order data saved to {orders_output_path}")
    print(f"Order items data saved to {order_items_output_path}")
    
    # Clean up any remaining temporary batch files in the data directory
    print("Cleaning up intermediate order batch files...")
    for file_pattern in ["orders_batch_*.parquet", "order_batch_*.parquet", "order_items_batch_*.parquet"]:
        pattern_path = os.path.join("./data", file_pattern)
        try:
            for batch_file in glob.glob(pattern_path):
                if (os.path.exists(batch_file) and 
                    os.path.basename(batch_file) != os.path.basename(orders_output_path) and
                    os.path.basename(batch_file) != os.path.basename(order_items_output_path)):
                    print(f"Removing temporary file: {batch_file}")
                    os.remove(batch_file)
        except Exception as e:
            print(f"Error cleaning up temporary files: {e}")
    
    print_memory_usage("After order generation")
    
    return orders_output_path, order_items_output_path

def worker_generate_reviews(args):
    """
    Worker function to generate review data in parallel
    
    Parameters:
        args: Tuple containing:
            - batch_id: ID of the current batch
            - start_idx: Starting index
            - batch_size: Size of the batch
            - product_ids: Array of product IDs
            - customer_ids: Array of customer IDs
            - product_customer_pairs: List of (product_id, customer_id) tuples
            - review_config: Dictionary with review generation configuration
            
    Returns:
        str: Path to the temporary file with generated reviews
    """
    batch_id, start_idx, batch_size, product_ids, customer_ids, product_customer_pairs, review_config = args
    
    start_time = time.time()
    print(f"Process {os.getpid()}: Starting review batch #{batch_id+1} generation ({batch_size} reviews)")
    
    # Get configuration
    positive_titles = review_config['positive_titles']
    neutral_titles = review_config['neutral_titles']
    negative_titles = review_config['negative_titles']
    rating_probabilities = review_config['rating_probabilities']
    
    # Set batch parameters
    num_chunks = min(10, batch_size // 10000 + 1)  # Split large batches into chunks
    chunk_size = batch_size // num_chunks
    
    # Prepare output file
    create_directory_if_not_exists('./data')
    output_file = os.path.join("./data", f"reviews_batch_{batch_id}.parquet")
    
    writer = None
    total_rows = 0
    
    for chunk_idx in range(num_chunks):
        # Calculate current chunk size
        current_chunk_start = chunk_idx * chunk_size
        current_chunk_end = min(current_chunk_start + chunk_size, batch_size)
        current_chunk_size = current_chunk_end - current_chunk_start
        
        if current_chunk_size <= 0:
            break
        
        # Generate a mix of reviews based on actual product-customer relationships
        # and random product-customer pairs
        if product_customer_pairs and random.random() < 0.7:
            # Use actual product-customer relationships for 70% of reviews when available
            pairs_count = min(current_chunk_size, len(product_customer_pairs))
            pairs = random.sample(product_customer_pairs, pairs_count)
            selected_products, selected_customers = zip(*pairs)
            
            # Fill remaining with random selections if needed
            if pairs_count < current_chunk_size:
                remaining = current_chunk_size - pairs_count
                random_products = np.random.choice(product_ids, remaining)
                random_customers = np.random.choice(customer_ids, remaining)
                
                selected_products = np.concatenate([selected_products, random_products])
                selected_customers = np.concatenate([selected_customers, random_customers])
        else:
            # Use random product-customer pairs
            selected_products = np.random.choice(product_ids, current_chunk_size)
            selected_customers = np.random.choice(customer_ids, current_chunk_size)
        
        # Generate ratings with a bias toward higher ratings
        ratings = np.random.choice([1, 2, 3, 4, 5], current_chunk_size, p=rating_probabilities)
        
        # Generate review titles based on ratings
        review_titles = []
        for rating in ratings:
            if rating >= 4:
                review_titles.append(random.choice(positive_titles))
            elif rating == 3:
                review_titles.append(random.choice(neutral_titles))
            else:
                review_titles.append(random.choice(negative_titles))
        
        # Generate review text
        review_texts = []
        for rating in ratings:
            if rating >= 4:
                # Positive reviews tend to be shorter
                sentences = random.randint(1, 5)
                review_texts.append(" ".join([
                    "This product is excellent." if i == 0 else
                    random.choice([
                        "I really love it.", 
                        "Would definitely buy again.", 
                        "Highly recommended.",
                        "Great value for money.",
                        "Exceeded my expectations.",
                        "Works perfectly for my needs.",
                        "The quality is outstanding.",
                        "Super happy with this purchase.",
                        "Best product I've bought in a while.",
                        "Exactly what I was looking for."
                    ])
                    for i in range(sentences)
                ]))
            elif rating == 3:
                # Neutral reviews are brief
                sentences = random.randint(1, 3)
                review_texts.append(" ".join([
                    "This product is okay." if i == 0 else
                    random.choice([
                        "It works as expected.",
                        "Good enough for the price.",
                        "Not exceptional but decent.",
                        "Meets basic requirements.",
                        "Has pros and cons.",
                        "Average quality.",
                        "Could be better but acceptable.",
                        "Works but not impressed.",
                        "It's fine for occasional use.",
                        "Just okay, nothing special."
                    ])
                    for i in range(sentences)
                ]))
            else:
                # Negative reviews tend to be longer
                sentences = random.randint(2, 8)
                review_texts.append(" ".join([
                    "Very disappointed with this product." if i == 0 else
                    random.choice([
                        "Would not recommend.",
                        "Poor quality for the price.",
                        "Stopped working after a few days.",
                        "Does not match the description.",
                        "Had to return it.",
                        "Save your money and buy something else.",
                        "Waste of money.",
                        "Customer service was unhelpful.",
                        "Several issues with this product.",
                        "Definitely not worth the price.",
                        "Expected much better quality.",
                        "Very frustrated with this purchase.",
                        "The worst product I've bought in a while."
                    ])
                    for i in range(sentences)
                ]))
        
        # Generate review dates
        # Reviews from the past 2 years
        min_date = datetime.now() - timedelta(days=730)
        max_date = datetime.now()
        
        # Generate random dates
        days_range = (max_date - min_date).days
        random_days = np.random.randint(0, days_range, current_chunk_size)
        review_dates = np.array([min_date + timedelta(days=int(days)) for days in random_days])
        
        # Generate helpful votes
        helpful_votes = np.random.geometric(p=0.5, size=current_chunk_size) - 1
        # Higher rated products tend to get more helpful votes
        helpful_votes = (helpful_votes * (ratings / 3)).astype(int)
        
        # Generate verified purchase flags (80% are verified)
        verified_purchases = np.random.choice([True, False], current_chunk_size, p=[0.8, 0.2])
        
        # Create data dictionary
        reviews_data = {
            'product_id': selected_products,
            'customer_id': selected_customers,
            'rating': ratings,
            'title': review_titles,
            'review_text': review_texts,
            'review_date': review_dates,
            'helpful_votes': helpful_votes,
            'verified_purchase': verified_purchases,
        }
        
        # Create PyArrow table directly
        pyarrow_arrays = []
        pyarrow_names = []
        
        for name, values in reviews_data.items():
            pyarrow_arrays.append(pa.array(values))
            pyarrow_names.append(name)
        
        reviews_table = pa.Table.from_arrays(pyarrow_arrays, names=pyarrow_names)
        
        # Write to output file
        if writer is None:
            # First chunk - create the writer
            writer = pq.ParquetWriter(
                output_file,
                reviews_table.schema,
                compression=None,  # No compression
                use_dictionary=True,
                write_statistics=True
            )
        
        writer.write_table(reviews_table)
        total_rows += len(reviews_table)
        
        # Clean up memory after each chunk
        del reviews_table
        gc.collect()
        
        # Log progress
        if chunk_idx % 5 == 0 or chunk_idx == num_chunks - 1:
            print(f"Process {os.getpid()}: Processed {chunk_idx+1}/{num_chunks} chunks " +
                  f"({total_rows} reviews, {(chunk_idx+1)/num_chunks*100:.1f}% complete)")
    
    # Close the writer
    if writer:
        writer.close()
    
    # Measure processing time
    proc_time = time.time() - start_time
    print(f"Process {os.getpid()} completed review batch #{batch_id+1} data generation in {proc_time:.2f} seconds")
    print(f"Generated {total_rows} reviews in file {output_file}")
    
    return output_file

def generate_review_data(product_df, customer_df, orders_df, num_reviews=50000):
    """
    Generate review data using parallel processing
    
    Parameters:
        product_df (str or PyArrow Table): Product data or file path
        customer_df (str or PyArrow Table): Customer data or file path
        orders_df (str or PyArrow Table): Order data or file path, used to extract product-customer relationships
        num_reviews (int): Number of reviews to generate
        
    Returns:
        str: Path to the generated review data file
    """
    print(f"Using {NUM_PROCESSES} processes to generate {num_reviews} reviews in parallel...")
    
    # If paths are provided, load the data using PyArrow
    if isinstance(product_df, str):
        product_table = pq.read_table(product_df)
        product_ids = product_table.column('product_id').to_numpy()
    else:
        # Assume it's already a PyArrow Table
        product_table = product_df
        product_ids = product_table.column('product_id').to_numpy()
    
    if isinstance(customer_df, str):
        customer_table = pq.read_table(customer_df)
        customer_ids = customer_table.column('customer_id').to_numpy()
    else:
        # Assume it's already a PyArrow Table
        customer_table = customer_df
        customer_ids = customer_table.column('customer_id').to_numpy()
    
    print(f"Available product count: {len(product_ids)}, available customer count: {len(customer_ids)}")
    
    # Extract product-customer relationships
    product_customer_pairs = []
    
    if isinstance(orders_df, str):
        orders_table = pq.read_table(orders_df)
    else:
        orders_table = orders_df
    
    # Check if order_id column exists in orders table
    if 'order_id' in orders_table.column_names:
        # Read order item data
        try:
            order_items_file = os.path.join("./data", "ecommerce_order_items.parquet")
            if os.path.exists(order_items_file):
                order_items_table = pq.read_table(order_items_file)
                
                # Extract product_id and order_id pairs from order items
                order_item_pairs = list(zip(
                    order_items_table.column('order_id').to_numpy(),
                    order_items_table.column('product_id').to_numpy()
                ))
                
                # Extract order_id and customer_id pairs from orders
                order_customer_pairs = list(zip(
                    orders_table.column('order_id').to_numpy(),
                    orders_table.column('customer_id').to_numpy()
                ))
                
                # Create lookup dictionaries
                order_to_customer = {order_id: cust_id for order_id, cust_id in order_customer_pairs}
                
                # Create product-customer pairs
                for order_id, prod_id in order_item_pairs:
                    if order_id in order_to_customer:
                        product_customer_pairs.append((prod_id, order_to_customer[order_id]))
                
                # Remove duplicates by converting to set and back to list
                product_customer_pairs = list(set(product_customer_pairs))
                
                print(f"Extracted {len(product_customer_pairs)} unique product-customer relationships from order data")
                
                # Release memory
                del order_items_table, order_item_pairs, order_customer_pairs, order_to_customer
                _ = gc.collect()
        except Exception as e:
            print(f"Error extracting product-customer relationships from order data: {e}")
            print("Using random product-customer combinations")
    
    # Set preset configuration data
    review_config = {
        'positive_titles': [
            "Love this product!", "Exceeded expectations", "Highly recommend",
            "Absolutely worth it", "Great quality", "Surprisingly good",
            "Perfect for my needs", "Glad I bought this", "Very satisfied",
            "Excellent quality", "Love it so much", "Great value",
            "Very practical", "Flawless", "Fantastic product"
        ],
        'neutral_titles': [
            "It's okay", "Meets expectations", "Satisfactory", "Average", 
            "Acceptable", "Decent quality", "Standard product", "Fair price", 
            "Basic functionality", "Good enough", "Reasonable quality"
        ],
        'negative_titles': [
            "Not recommended", "Disappointed", "Waste of money", 
            "Below expectations", "Poor quality", "Regret purchasing", 
            "Better options available", "Not worth the price", "Has issues",
            "Doesn't work well", "Save your money"
        ],
        'rating_probabilities': [0.05, 0.10, 0.15, 0.30, 0.40]  # Probability of 1-5 star ratings
    }
    
    # Determine batch size and number of batches
    # Increase processing per batch
    batch_size = max(100000, num_reviews // (NUM_PROCESSES * 2))
    num_batches = (num_reviews + batch_size - 1) // batch_size
    
    print(f"Batch size: {batch_size}, total batch count: {num_batches}")
    
    # Prepare process pool parameters
    args_list = []
    for batch_id in range(num_batches):
        start_idx = batch_id * batch_size
        end_idx = min(start_idx + batch_size, num_reviews)
        batch_size_actual = end_idx - start_idx
        
        if batch_size_actual <= 0:
            break
            
        args_list.append((
            batch_id, 
            start_idx,
            batch_size_actual,
            product_ids,
            customer_ids,
            product_customer_pairs,
            review_config
        ))
    
    # Create process pool
    start_time = time.time()
    
    with Pool(processes=NUM_PROCESSES, 
              initializer=init_worker, 
              initargs=(RANDOM_SEED + 4000,)) as pool:
        # Execute parallel processing
        temp_files = pool.map(worker_generate_reviews, args_list)
    
    pool_time = time.time() - start_time
    print(f"All review data generation completed, parallel processing took {pool_time:.2f} seconds")
    
    # Filter out None results
    temp_files = [f for f in temp_files if f]
    
    if not temp_files:
        print("Warning: No review data batches were generated!")
        return None
    
    # Merge all temporary files
    output_file = os.path.join("./data", "ecommerce_reviews.parquet")
    
    print(f"Merging {len(temp_files)} temporary review files...")
    start_merge = time.time()
    
    # Ensure output directory exists
    create_directory_if_not_exists('./data')
    
    total_rows = merge_files(temp_files, output_file)
    merge_time = time.time() - start_merge
    
    print(f"Merge completed, took {merge_time:.2f} seconds")
    print(f"Review data generation completed: {total_rows} review records written to {output_file}")
    
    # Clean up any remaining temporary batch files
    print("Cleaning up intermediate batch files...")
    for file_pattern in ["reviews_batch_*.parquet", "review_batch_*.parquet"]:
        pattern_path = os.path.join("./data", file_pattern)
        try:
            for batch_file in glob.glob(pattern_path):
                if os.path.exists(batch_file) and os.path.basename(batch_file) != os.path.basename(output_file):
                    print(f"Removing temporary file: {batch_file}")
                    os.remove(batch_file)
        except Exception as e:
            print(f"Error cleaning up temporary files: {e}")
    
    return output_file

def generate_inventory_data(product_df, num_warehouses=20):
    """
    Generate inventory data for e-commerce simulation
    
    Parameters:
        product_df: PyArrow Table or file path containing product data
        num_warehouses: Number of warehouses to simulate (default: 20)
    
    Returns:
        Path to the generated parquet file
    """
    print(f"Generating inventory data...")
    
    # If path is provided, load the data using PyArrow
    if isinstance(product_df, str):
        product_table = pq.read_table(product_df)
    else:
        # Assume it's already a PyArrow Table
        product_table = product_df
    
    product_count = product_table.num_rows
    print(f"Generating inventory data for {product_count} products across {num_warehouses} warehouses...")
    
    # Get product IDs
    product_ids = product_table.column('product_id').to_numpy()
    
    # Extract a mapping of product_ids to product_names
    product_names = {}
    for i in range(product_count):
        product_id = product_table.column('product_id')[i].as_py()
        product_name = product_table.column('product_name')[i].as_py()
        product_names[product_id] = product_name
    
    # Define warehouse cities - ensure enough cities for the requested number of warehouses
    warehouse_cities = [
        "New York", "Los Angeles", "Chicago", "Houston", "Phoenix", 
        "Philadelphia", "San Antonio", "San Diego", "Dallas", "San Jose",
        "Austin", "Jacksonville", "Fort Worth", "Columbus", "Indianapolis",
        "Charlotte", "San Francisco", "Seattle", "Denver", "Washington DC",
        "Boston", "Nashville", "Baltimore", "Oklahoma City", "Portland",
        "Las Vegas", "Milwaukee", "Albuquerque", "Tucson", "Fresno",
        "Sacramento", "Long Beach", "Kansas City", "Mesa", "Atlanta",
        "Omaha", "Raleigh", "Miami", "Oakland", "Minneapolis",
        "Tulsa", "Cleveland", "New Orleans", "Honolulu", "St. Louis",
        "Pittsburgh", "Cincinnati", "Orlando", "Buffalo", "Rochester",
        "Salt Lake City", "Detroit", "Memphis", "Louisville", "Tampa",
        "Hartford", "Richmond", "Providence", "Des Moines", "Boise"
    ]
    
    # Ensure we have enough cities for the number of warehouses
    if num_warehouses > len(warehouse_cities):
        # Generate additional cities if needed
        more_cities_needed = num_warehouses - len(warehouse_cities)
        extra_cities = [f"Distribution Center {i+1}" for i in range(more_cities_needed)]
        warehouse_cities.extend(extra_cities)
    
    # Generate warehouse information
    warehouse_ids = [f"W{i+1:03d}" for i in range(num_warehouses)]
    warehouse_names = [f"Warehouse {city}" for city in warehouse_cities[:num_warehouses]]
    
    # Generate warehouse addresses - ensure enough addresses
    base_addresses = [
        "123 Main St, New York, NY", 
        "456 Market St, Los Angeles, CA",
        "789 Oak St, Chicago, IL",
        "101 Pine St, Houston, TX",
        "202 Maple St, Phoenix, AZ",
        "303 Cedar St, Philadelphia, PA",
        "404 Birch St, San Antonio, TX",
        "505 Elm St, San Diego, CA",
        "606 Walnut St, Dallas, TX",
        "707 Cherry St, San Jose, CA",
        "808 Spruce St, Austin, TX",
        "909 Ash St, Jacksonville, FL",
        "1010 Poplar St, Fort Worth, TX",
        "1111 Sycamore St, Columbus, OH",
        "1212 Willow St, Indianapolis, IN",
        "1313 Fir St, Charlotte, NC",
        "1414 Redwood St, San Francisco, CA",
        "1515 Sequoia St, Seattle, WA",
        "1616 Pine St, Denver, CO",
        "1717 Constitution Ave, Washington, DC"
    ]
    
    # Generate additional addresses if needed
    if num_warehouses > len(base_addresses):
        more_addresses_needed = num_warehouses - len(base_addresses)
        street_names = ["Industrial", "Commerce", "Distribution", "Logistics", "Supply", "Warehouse", "Storage", "Shipping"]
        extra_addresses = []
        
        for i in range(more_addresses_needed):
            street_name = random.choice(street_names)
            number = random.randint(100, 9999)
            city = warehouse_cities[len(base_addresses) + i]
            state_code = "".join(random.choices("ABCDEFGHIJKLMNOPQRSTUVWXYZ", k=2))
            extra_addresses.append(f"{number} {street_name} Blvd, {city}, {state_code}")
        
        base_addresses.extend(extra_addresses)
    
    warehouse_addresses = base_addresses[:num_warehouses]
    
    # Create temporary directory for batch files
    temp_dir = os.path.join(os.getcwd(), "temp_inventory")
    create_directory_if_not_exists(temp_dir)
    
    # Split into batches for better progress tracking
    batch_size = 100000
    num_batches = (len(product_ids) + batch_size - 1) // batch_size
    
    temp_files = []
    
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(product_ids))
        batch_product_ids = product_ids[start_idx:end_idx]
        
        print(f"Processing inventory batch {batch_idx+1}/{num_batches} ({len(batch_product_ids)} products)")
        
        inventory_records = []
        
        for product_id in batch_product_ids:
            # Each product is stocked in 1-10 random warehouses, but not more than what's available
            max_warehouses = min(10, num_warehouses)
            num_warehouses_for_product = random.randint(1, max_warehouses)
            selected_warehouses = random.sample(range(num_warehouses), num_warehouses_for_product)
            
            # Get product name
            product_name = product_names.get(product_id, f"Unknown Product {product_id}")
            
            for warehouse_idx in selected_warehouses:
                warehouse_id = warehouse_ids[warehouse_idx]
                warehouse_name = warehouse_names[warehouse_idx]
                warehouse_address = warehouse_addresses[warehouse_idx]
                
                # Generate inventory quantities and status
                quantity = random.randint(0, 500)
                if quantity == 0:
                    status = 'Out of Stock'
                elif quantity < 10:
                    status = 'Low Stock'
                else:
                    status = 'In Stock'
                
                # Calculate shelf location
                aisle = random.randint(1, 50)
                shelf = random.choice(['A', 'B', 'C', 'D', 'E'])
                position = random.randint(1, 20)
                shelf_location = f"{aisle:02d}-{shelf}-{position:02d}"
                
                # Last updated date (within the past month)
                days_ago = random.randint(0, 30)
                last_updated = datetime.now() - timedelta(days=days_ago)
                
                # Minimum and maximum stock levels
                min_stock = random.randint(5, 25)
                max_stock = random.randint(100, 500)
                
                # Reorder level and quantity
                reorder_level = random.randint(min_stock, min_stock * 2)
                reorder_quantity = random.randint(max_stock // 4, max_stock // 2)
                
                inventory_records.append({
                    'inventory_id': f"INV-{product_id}-{warehouse_id}",
                    'product_id': product_id,
                    'product_name': product_name,
                    'warehouse_id': warehouse_id,
                    'warehouse_name': warehouse_name,
                    'warehouse_address': warehouse_address,
                    'quantity': quantity,
                    'status': status,
                    'shelf_location': shelf_location,
                    'last_updated': last_updated,
                    'min_stock': min_stock,
                    'max_stock': max_stock,
                    'reorder_level': reorder_level,
                    'reorder_quantity': reorder_quantity
                })
        
        if inventory_records:
            # Create PyArrow table directly
            pyarrow_arrays = []
            pyarrow_names = []
            
            # Group data by column
            columns = {}
            for record in inventory_records:
                for key, value in record.items():
                    if key not in columns:
                        columns[key] = []
                    columns[key].append(value)
            
            # Create PyArrow arrays
            for name, values in columns.items():
                pyarrow_arrays.append(pa.array(values))
                pyarrow_names.append(name)
            
            inventory_table = pa.Table.from_arrays(pyarrow_arrays, names=pyarrow_names)
            
            # Write batch to temporary file
            batch_file = os.path.join(temp_dir, f"inventory_batch_{batch_idx}.parquet")
            pq.write_table(inventory_table, batch_file, compression=None)
            
            temp_files.append(batch_file)
            print(f"Wrote {len(inventory_records)} inventory records to {batch_file}")
    
    # Ensure output directory exists
    create_directory_if_not_exists('./data')
    
    # Merge temporary files
    output_path = './data/ecommerce_inventory.parquet'
    
    if temp_files:
        total_rows = merge_files(temp_files, output_path)
        
        # Clean up temporary directory
        try:
            shutil.rmtree(temp_dir)
            print(f"Removed temporary directory: {temp_dir}")
        except Exception as e:
            print(f"Could not remove temporary directory: {e}")
        
        print(f"Generated {total_rows} inventory records")
        print(f"Inventory data saved to {output_path}")
    else:
        print("Warning: No inventory data was generated!")
    
    # Clean up any remaining temporary batch files in the data directory
    print("Cleaning up intermediate inventory batch files...")
    for file_pattern in ["inventory_batch_*.parquet"]:
        pattern_path = os.path.join("./data", file_pattern)
        try:
            for batch_file in glob.glob(pattern_path):
                if os.path.exists(batch_file) and os.path.basename(batch_file) != os.path.basename(output_path):
                    print(f"Removing temporary file: {batch_file}")
                    os.remove(batch_file)
        except Exception as e:
            print(f"Error cleaning up temporary files: {e}")
    
    print_memory_usage("After inventory generation")
    
    return output_path

def worker_generate_customers(args):
    """
    Worker function to generate customer data in parallel
    
    Parameters:
        args: Tuple containing (batch_id, start_idx, batch_size, countries, weights) 
            batch_id: Batch identifier
            start_idx: Starting index for customer IDs
            batch_size: Number of customers to generate in this batch
            countries: List of countries to sample from
            weights: Probability weights for country selection
            
    Returns:
        str: Path to temporary file with customer data
    """
    batch_id, start_idx, batch_size, countries, weights = args
    
    print(f"Process {os.getpid()}: Generating customer batch #{batch_id+1} ({batch_size} customers)")
    start_time = time.time()
    
    # Generate customer IDs
    customer_ids = [f"C{start_idx+i:09d}" for i in range(batch_size)]
    
    # First names and last names
    first_names = [
        "James", "John", "Robert", "Michael", "William", "David", "Richard", "Joseph", "Thomas", "Charles",
        "Mary", "Patricia", "Jennifer", "Linda", "Elizabeth", "Barbara", "Susan", "Jessica", "Sarah", "Karen",
        "Christopher", "Daniel", "Matthew", "Anthony", "Mark", "Donald", "Steven", "Paul", "Andrew", "Joshua",
        "Emily", "Emma", "Madison", "Olivia", "Hannah", "Abigail", "Isabella", "Samantha", "Elizabeth", "Ashley",
        "Kenneth", "George", "Brian", "Edward", "Ronald", "Timothy", "Jason", "Jeffrey", "Ryan", "Jacob",
        "Nicole", "Amanda", "Stephanie", "Melissa", "Rebecca", "Laura", "Anna", "Taylor", "Megan", "Victoria"
    ]
    
    last_names = [
        "Smith", "Johnson", "Williams", "Jones", "Brown", "Davis", "Miller", "Wilson", "Moore", "Taylor",
        "Anderson", "Thomas", "Jackson", "White", "Harris", "Martin", "Thompson", "Garcia", "Martinez", "Robinson",
        "Clark", "Rodriguez", "Lewis", "Lee", "Walker", "Hall", "Allen", "Young", "Hernandez", "King",
        "Wright", "Lopez", "Hill", "Scott", "Green", "Adams", "Baker", "Gonzalez", "Nelson", "Carter",
        "Mitchell", "Perez", "Roberts", "Turner", "Phillips", "Campbell", "Parker", "Evans", "Edwards", "Collins"
    ]
    
    # Generate customer data vectors
    first_name_indices = np.random.randint(0, len(first_names), batch_size)
    last_name_indices = np.random.randint(0, len(last_names), batch_size)
    
    generated_first_names = [first_names[i] for i in first_name_indices]
    generated_last_names = [last_names[i] for i in last_name_indices]
    
    # Email domains
    email_domains = ["gmail.com", "yahoo.com", "hotmail.com", "outlook.com", "icloud.com", 
                     "aol.com", "mail.com", "protonmail.com", "zoho.com", "yandex.com"]
    
    # Generate emails based on name
    emails = []
    for i in range(batch_size):
        first = generated_first_names[i].lower()
        last = generated_last_names[i].lower()
        domain = random.choice(email_domains)
        
        # Different email patterns
        pattern = random.randint(1, 5)
        if pattern == 1:
            email = f"{first}.{last}@{domain}"
        elif pattern == 2:
            email = f"{first[0]}{last}@{domain}"
        elif pattern == 3:
            email = f"{first}_{last}@{domain}"
        elif pattern == 4:
            email = f"{last}.{first}@{domain}"
        else:
            email = f"{first}{last[0]}@{domain}"
            
        # Add random number to email if necessary
        if random.random() < 0.3:
            num = random.randint(1, 999)
            email = f"{email.split('@')[0]}{num}@{domain}"
            
        emails.append(email)
    
    # Generate countries based on weight distribution
    selected_countries = np.random.choice(countries, batch_size, p=weights)
    
    # Generate cities
    cities = {
        "United States": ["New York", "Los Angeles", "Chicago", "Houston", "Phoenix", "Philadelphia", "San Antonio", "San Diego", "Dallas", "San Jose"],
        "Canada": ["Toronto", "Montreal", "Vancouver", "Calgary", "Edmonton", "Ottawa", "Winnipeg", "Quebec City", "Hamilton", "Kitchener"],
        "United Kingdom": ["London", "Birmingham", "Manchester", "Glasgow", "Liverpool", "Bristol", "Edinburgh", "Sheffield", "Leeds", "Leicester"],
        "Germany": ["Berlin", "Hamburg", "Munich", "Cologne", "Frankfurt", "Stuttgart", "Dsseldorf", "Leipzig", "Dortmund", "Essen"],
        "France": ["Paris", "Marseille", "Lyon", "Toulouse", "Nice", "Nantes", "Strasbourg", "Montpellier", "Bordeaux", "Lille"],
        "Japan": ["Tokyo", "Yokohama", "Osaka", "Nagoya", "Sapporo", "Fukuoka", "Kobe", "Kyoto", "Kawasaki", "Saitama"],
        "Australia": ["Sydney", "Melbourne", "Brisbane", "Perth", "Adelaide", "Gold Coast", "Canberra", "Newcastle", "Wollongong", "Logan City"],
        "Brazil": ["Sao Paulo", "Rio de Janeiro", "Brasilia", "Salvador", "Fortaleza", "Belo Horizonte", "Manaus", "Curitiba", "Recife", "Porto Alegre"],
        "China": ["Shanghai", "Beijing", "Guangzhou", "Shenzhen", "Tianjin", "Chongqing", "Wuhan", "Dongguan", "Chengdu", "Nanjing"],
        "India": ["Mumbai", "Delhi", "Bangalore", "Hyderabad", "Ahmedabad", "Chennai", "Kolkata", "Surat", "Pune", "Jaipur"]
    }
    
    selected_cities = []
    for country in selected_countries:
        selected_cities.append(random.choice(cities.get(country, ["Unknown City"])))
    
    # Generate addresses
    street_names = [
        "Main St", "High St", "Park Ave", "Oak St", "Pine St", "Maple Ave", "Cedar Dr", "Elm St", 
        "Washington St", "Lake Ave", "Hill Rd", "River Rd", "Church St", "Meadow Ln", "Valley Dr"
    ]
    
    addresses = []
    for i in range(batch_size):
        num = random.randint(1, 9999)
        street = random.choice(street_names)
        addresses.append(f"{num} {street}")
    
    # Generate postal codes based on country format
    postal_codes = []
    for country in selected_countries:
        if country == "United States":
            postal_codes.append(f"{random.randint(10000, 99999):05d}")
        elif country == "Canada":
            letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            postal_codes.append(f"{random.choice(letters)}{random.randint(0, 9)}{random.choice(letters)} {random.randint(0, 9)}{random.choice(letters)}{random.randint(0, 9)}")
        elif country == "United Kingdom":
            letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            postal_codes.append(f"{random.choice(letters)}{random.choice(letters)}{random.randint(0, 9)} {random.randint(0, 9)}{random.choice(letters)}{random.choice(letters)}")
        elif country == "Germany" or country == "France":
            postal_codes.append(f"{random.randint(10000, 99999):05d}")
        elif country == "Japan":
            postal_codes.append(f"{random.randint(100, 999)}-{random.randint(1000, 9999):04d}")
        elif country == "Australia":
            postal_codes.append(f"{random.randint(1000, 9999):04d}")
        elif country == "Brazil":
            postal_codes.append(f"{random.randint(10000, 99999)}-{random.randint(100, 999):03d}")
        elif country == "China":
            postal_codes.append(f"{random.randint(100000, 999999):06d}")
        elif country == "India":
            postal_codes.append(f"{random.randint(100000, 999999):06d}")
        else:
            postal_codes.append(f"{random.randint(10000, 99999):05d}")
    
    # Generate phone numbers
    phone_numbers = []
    for country in selected_countries:
        if country == "United States" or country == "Canada":
            area = random.randint(100, 999)
            prefix = random.randint(100, 999)
            line = random.randint(1000, 9999)
            phone_numbers.append(f"+1 ({area}) {prefix}-{line}")
        elif country == "United Kingdom":
            phone_numbers.append(f"+44 {random.randint(1000, 9999)} {random.randint(100000, 999999)}")
        elif country == "Germany":
            phone_numbers.append(f"+49 {random.randint(100, 999)} {random.randint(1000000, 9999999)}")
        elif country == "France":
            phone_numbers.append(f"+33 {random.randint(1, 9)} {random.randint(10, 99)} {random.randint(10, 99)} {random.randint(10, 99)} {random.randint(10, 99)}")
        elif country == "Japan":
            phone_numbers.append(f"+81 {random.randint(10, 99)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}")
        elif country == "Australia":
            phone_numbers.append(f"+61 {random.randint(2, 9)} {random.randint(1000, 9999)} {random.randint(1000, 9999)}")
        elif country == "Brazil":
            phone_numbers.append(f"+55 {random.randint(10, 99)} {random.randint(10000, 99999)}-{random.randint(1000, 9999)}")
        elif country == "China":
            phone_numbers.append(f"+86 {random.randint(100, 999)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}")
        elif country == "India":
            phone_numbers.append(f"+91 {random.randint(70, 99)} {random.randint(1000, 9999)} {random.randint(1000, 9999)}")
        else:
            phone_numbers.append(f"+{random.randint(1, 999)} {random.randint(1000, 9999)} {random.randint(1000, 9999)}")
    
    # Registration dates - past 5 years
    now = datetime.now()
    five_years_ago = now - timedelta(days=365*5)
    
    registration_timestamps = np.random.uniform(
        time.mktime(five_years_ago.timetuple()),
        time.mktime(now.timetuple()),
        batch_size
    )
    registration_dates = [datetime.fromtimestamp(ts) for ts in registration_timestamps]
    
    # Account status (active/inactive)
    account_statuses = np.random.choice(
        ["Active", "Inactive"], 
        batch_size, 
        p=[0.95, 0.05]
    )
    
    # Loyalty tiers
    loyalty_tier_choices = ["Bronze", "Silver", "Gold", "Platinum", "Diamond"]
    loyalty_tier_weights = [0.5, 0.25, 0.15, 0.07, 0.03]  # Most users are bronze
    loyalty_tiers = np.random.choice(
        loyalty_tier_choices,
        batch_size,
        p=loyalty_tier_weights
    )
    
    # Create a dictionary of arrays
    customer_data = {
        'customer_id': customer_ids,
        'first_name': generated_first_names,
        'last_name': generated_last_names,
        'email': emails,
        'phone_number': phone_numbers,
        'address': addresses,
        'city': selected_cities,
        'postal_code': postal_codes,
        'country': selected_countries,
        'registration_date': registration_dates,
        'account_status': account_statuses,
        'loyalty_tier': loyalty_tiers
    }
    
    # Create PyArrow table directly
    pyarrow_arrays = []
    pyarrow_names = []
    
    for name, values in customer_data.items():
        pyarrow_arrays.append(pa.array(values))
        pyarrow_names.append(name)
    
    customer_table = pa.Table.from_arrays(pyarrow_arrays, names=pyarrow_names)
    
    # Create output file
    temp_file = os.path.join("./data", f"customers_batch_{batch_id}.parquet")
    create_directory_if_not_exists("./data")
    
    # Write to parquet with no compression
    pq.write_table(customer_table, temp_file, compression=None)
    
    proc_time = time.time() - start_time
    print(f"Process {os.getpid()}: Generated {batch_size} customers in {proc_time:.2f} seconds")
    print(f"Saved to {temp_file}")
    
    return temp_file

def generate_customer_data(num_customers=10000):
    """
    Generate customer data using parallel processing
    
    Parameters:
        num_customers (int): Number of customers to generate
        
    Returns:
        str: Path to the generated customer data file
    """
    print(f"Generating {num_customers} customers using {NUM_PROCESSES} processes...")
    
    # Define countries and weights (more customers from US, fewer from other regions)
    countries = [
        "United States", "Canada", "United Kingdom", "Germany", 
        "France", "Japan", "Australia", "Brazil", "China", "India"
    ]
    weights = [0.40, 0.08, 0.08, 0.07, 0.07, 0.06, 0.06, 0.06, 0.06, 0.06]
    
    # Determine batch size and number of batches
    batch_size = max(10000, num_customers // (NUM_PROCESSES * 2))
    num_batches = (num_customers + batch_size - 1) // batch_size
    
    print(f"Using batch size of {batch_size}, will create {num_batches} batches")
    
    # Prepare batch parameters
    args_list = []
    for batch_id in range(num_batches):
        start_idx = batch_id * batch_size
        end_idx = min(start_idx + batch_size, num_customers)
        batch_size_actual = end_idx - start_idx
        
        if batch_size_actual <= 0:
            break
            
        args_list.append((
            batch_id,
            start_idx,
            batch_size_actual,
            countries,
            weights
        ))
    
    # Create process pool
    start_time = time.time()
    
    with Pool(processes=NUM_PROCESSES, initializer=init_worker, initargs=(RANDOM_SEED,)) as pool:
        temp_files = pool.map(worker_generate_customers, args_list)
    
    pool_time = time.time() - start_time
    print(f"All customer data generation completed in {pool_time:.2f} seconds")
    
    # Merge all temporary files
    output_file = os.path.join("./data", "ecommerce_customers.parquet")
    
    print(f"Merging {len(temp_files)} temporary customer files...")
    start_merge = time.time()
    
    # Ensure output directory exists
    create_directory_if_not_exists('./data')
    
    total_rows = merge_files(temp_files, output_file)
    merge_complete_time = time.time() - start_merge
    
    print(f"Merge completed in {merge_complete_time:.2f} seconds")
    print(f"Customer data generation completed: {total_rows} customer records written to {output_file}")
    
    # Clean up any remaining temporary batch files
    print("Cleaning up intermediate customer batch files...")
    for file_pattern in ["customers_batch_*.parquet", "customer_batch_*.parquet"]:
        pattern_path = os.path.join("./data", file_pattern)
        try:
            for batch_file in glob.glob(pattern_path):
                if os.path.exists(batch_file) and os.path.basename(batch_file) != os.path.basename(output_file):
                    print(f"Removing temporary file: {batch_file}")
                    os.remove(batch_file)
        except Exception as e:
            print(f"Error cleaning up temporary files: {e}")
    
    return output_file

def main():
    """
    Main function to generate e-commerce data based on user parameters
    """
    parser = argparse.ArgumentParser(description="Generate synthetic e-commerce data in Parquet format")
    
    # Data generation options
    parser.add_argument('--size', type=str, choices=['small', 'medium', 'large', 'xlarge'], default='small',
                      help='Size of the dataset to generate (small: ~100MB, medium: ~1GB, large: ~10GB, xlarge: ~100GB)')
    parser.add_argument('--seed', type=int, default=42,
                      help='Random seed for reproducibility')
    parser.add_argument('--scale-factor', type=float, default=1.0,
                      help='Scale factor to adjust dataset size')
    parser.add_argument('--output-dir', type=str, default='./data',
                      help='Directory to store the generated data')
    
    # Processing options
    parser.add_argument('--num-processes', type=int, default=multiprocessing.cpu_count(),
                      help='Number of processes to use for parallel generation')
    
    # Parse arguments
    args = parser.parse_args()
    
    # Set global parameters
    global RANDOM_SEED, NUM_PROCESSES
    RANDOM_SEED = args.seed
    NUM_PROCESSES = args.num_processes
    
    # Set target file sizes in MB
    target_sizes = {
        'small': 100,     # ~100 MB
        'medium': 1000,   # ~1 GB
        'large': 10000,   # ~10 GB
        'xlarge': 100000  # ~100 GB
    }
    
    # Set dataset size configuration based on size parameter and scale factor
    base_size_config = {
        'small': {
            'products': 5000,
            'customers': 25000,
            'orders': 50000,
            'order_items_per_order': 3,
            'reviews': 100000,
            'warehouses': 10
        },
        'medium': {
            'products': 50000,
            'customers': 250000,
            'orders': 500000, 
            'order_items_per_order': 5,
            'reviews': 2500000,
            'warehouses': 20
        },
        'large': {
            'products': 500000,
            'customers': 2500000,
            'orders': 5000000,
            'order_items_per_order': 8,
            'reviews': 25000000,
            'warehouses': 50
        },
        'xlarge': {
            'products': 5000000,
            'customers': 25000000,
            'orders': 50000000,
            'order_items_per_order': 12,
            'reviews': 250000000,
            'warehouses': 100
        }
    }
    
    # Apply scale factor
    size_config = {}
    for key, value in base_size_config[args.size].items():
        size_config[key] = int(value * args.scale_factor)
    
    # Create output directory if it doesn't exist
    create_directory_if_not_exists(args.output_dir)
    
    # Set random seed for reproducibility
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    
    # Print configuration
    print(f"\n{'=' * 80}")
    print(f"Generating {args.size.upper()} e-commerce dataset (Scale Factor: {args.scale_factor})")
    print(f"{'=' * 80}")
    print(f"Configuration:")
    print(f"  - Target size: ~{target_sizes[args.size]} MB")
    print(f"  - Products: {size_config['products']}")
    print(f"  - Customers: {size_config['customers']}")
    print(f"  - Orders: {size_config['orders']} (with avg {size_config['order_items_per_order']} items per order)")
    print(f"  - Reviews: {size_config['reviews']}")
    print(f"  - Warehouses: {size_config['warehouses']}")
    print(f"  - Processes: {NUM_PROCESSES}")
    print(f"  - Random seed: {RANDOM_SEED}")
    print(f"  - Output directory: {args.output_dir}")
    print(f"{'=' * 80}\n")
    
    # Start timer
    overall_start_time = time.time()
    
    # Generate data in sequence: products -> customers -> orders -> inventory -> reviews
    products_result = generate_product_data(num_products=size_config['products'])
    product_file = products_result[1]  # Get the file path from the tuple
    print_memory_usage("After product generation")
    
    customer_file = generate_customer_data(num_customers=size_config['customers'])
    print_memory_usage("After customer generation")
    
    orders_file, order_items_file = generate_order_data(
        product_file, 
        customer_file, 
        num_orders=size_config['orders'],
        avg_items_per_order=size_config['order_items_per_order']
    )
    print_memory_usage("After order generation")
    
    inventory_file = generate_inventory_data(
        product_file, 
        num_warehouses=size_config['warehouses']
    )
    print_memory_usage("After inventory generation")
    
    review_file = generate_review_data(
        product_file, 
        customer_file, 
        orders_file, 
        num_reviews=size_config['reviews']
    )
    print_memory_usage("After review generation")
    
    # Final cleanup of any leftover temporary files
    print("\nPerforming final cleanup of any temporary files...")
    # Define patterns for all possible temp files
    temp_patterns = [
        "reviews_batch_*.parquet", 
        "customers_batch_*.parquet", 
        "orders_batch_*.parquet", 
        "order_items_batch_*.parquet", 
        "inventory_batch_*.parquet",
        "products_batch_*.parquet",
        "*_batch_*.parquet"  # Catch-all pattern
    ]
    
    # Final output files to preserve
    final_files = [
        os.path.basename(product_file),
        os.path.basename(customer_file),
        os.path.basename(orders_file),
        os.path.basename(order_items_file),
        os.path.basename(inventory_file),
        os.path.basename(review_file)
    ]
    
    # Clean up all temp files
    files_deleted = 0
    for pattern in temp_patterns:
        pattern_path = os.path.join("./data", pattern)
        try:
            for temp_file in glob.glob(pattern_path):
                if os.path.exists(temp_file) and os.path.basename(temp_file) not in final_files:
                    print(f"Removing leftover temporary file: {temp_file}")
                    os.remove(temp_file)
                    files_deleted += 1
        except Exception as e:
            print(f"Error cleaning up temporary files with pattern {pattern}: {e}")
    
    if files_deleted > 0:
        print(f"Cleaned up {files_deleted} leftover temporary files")
    else:
        print("No leftover temporary files found")
    
    # Calculate total time
    overall_time = time.time() - overall_start_time
    print(f"\n{'=' * 80}")
    print(f"Data generation completed in {overall_time:.2f} seconds ({overall_time/60:.2f} minutes)")
    
    # Calculate and display actual file sizes
    files = [
        product_file, 
        customer_file, 
        orders_file, 
        order_items_file, 
        inventory_file, 
        review_file
    ]
    
    print("\nGenerated files:")
    total_size_bytes = 0
    
    for file_path in files:
        if file_path and os.path.exists(file_path):
            size_bytes = os.path.getsize(file_path)
            total_size_bytes += size_bytes
            size_mb = size_bytes / (1024 * 1024)
            print(f"  - {os.path.basename(file_path)}: {size_mb:.2f} MB")
    
    total_size_mb = total_size_bytes / (1024 * 1024)
    print(f"\nTotal disk storage: {total_size_mb:.2f} MB ({total_size_mb/1024:.2f} GB)")
    
    # Compare with target size
    target_mb = target_sizes[args.size]
    deviation = (total_size_mb - target_mb) / target_mb * 100
    print(f"Target size: {target_mb:.2f} MB")
    print(f"Deviation from target: {deviation:.2f}%")
    
    # Provide feedback if size is significantly off
    if abs(deviation) > 10:
        if deviation > 0:
            suggested_scale = args.scale_factor * (target_mb / total_size_mb)
            print(f"\nThe generated data is larger than the target size.")
            print(f"To get closer to the target size, try using --scale-factor {suggested_scale:.2f}")
        else:
            suggested_scale = args.scale_factor * (target_mb / total_size_mb)
            print(f"\nThe generated data is smaller than the target size.")
            print(f"To get closer to the target size, try using --scale-factor {suggested_scale:.2f}")
    
    print(f"{'=' * 80}")
    
    print("\nSample command to load the data in Python with PyArrow:")
    print("```python")
    print("import pyarrow.parquet as pq")
    print("# Load the datasets")
    print("products_table = pq.read_table('./data/ecommerce_products.parquet')")
    print("customers_table = pq.read_table('./data/ecommerce_customers.parquet')")
    print("orders_table = pq.read_table('./data/ecommerce_orders.parquet')")
    print("order_items_table = pq.read_table('./data/ecommerce_order_items.parquet')")
    print("inventory_table = pq.read_table('./data/ecommerce_inventory.parquet')")
    print("reviews_table = pq.read_table('./data/ecommerce_reviews.parquet')")
    print("```")
    
    return 0

if __name__ == "__main__":
    sys.exit(main()) 